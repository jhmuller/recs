{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537bd595",
   "metadata": {},
   "source": [
    "https://betterprogramming.pub/building-a-recommendation-engine-with-pytorch-d64be4856fe7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9817a",
   "metadata": {},
   "source": [
    "# Embeddings for Recommendations\n",
    "\n",
    "Here I use a move rating data set from Kaggle.\n",
    "In this notebook I will derive embeddings for users and movies\n",
    "and these will be the ingredients of a matrix factorization\n",
    "of the full rating matrix.\n",
    "\n",
    "We will train a simple Neural Net using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2313637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89d633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03f9b3",
   "metadata": {},
   "source": [
    "## According to the cuda documentation this can be helpful in debugging\n",
    "alhough I have not used it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb9dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f08c1",
   "metadata": {},
   "source": [
    "### Note: In case I decide to use the GPU card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1a4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device in use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e216cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data and show shape and head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6996373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26024289, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1425941529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1425942435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      110     1.0  1425941529\n",
       "1       1      147     4.5  1425942435"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origdf = pd.read_csv(\"ratings.csv\")\n",
    "print(origdf.shape)\n",
    "origdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33f3eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max userId 270896 num users 270896\n",
      "max movieId 176275 num movies 45115\n"
     ]
    }
   ],
   "source": [
    "print(f\"max userId {origdf.userId.max()} num users {origdf.userId.nunique()}\")\n",
    "print(f\"max movieId {origdf.movieId.max()} num movies {origdf.movieId.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43340bc",
   "metadata": {},
   "source": [
    "# Subsampling\n",
    "The original dataset was too large for my Cuda memory\n",
    "so I will subsample to a much saller set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "398b54ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max userId 9999 num users 9930\n",
      "max movieId 4999 num movies 4783\n",
      "(676352, 4)\n"
     ]
    }
   ],
   "source": [
    "subsample = True\n",
    "df = origdf.copy()\n",
    "if subsample:\n",
    "    max_user =  10000\n",
    "    max_movie = 5000\n",
    "    df = df.loc[df.movieId < max_movie]\n",
    "    df = df.loc[df.userId < max_user]\n",
    "    print(f\"max userId {df.userId.max()} num users {df.userId.nunique()}\")\n",
    "    print(f\"max movieId {df.movieId.max()} num movies {df.movieId.nunique()}\")    \n",
    "    print(df.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978048f",
   "metadata": {},
   "source": [
    "## Partition the dataset\n",
    "into train, val and test.\n",
    "We can use val to tune the hyper parameters.\n",
    "\n",
    "Test will be held out until all training is done.\n",
    "\n",
    "We could use the sklearn function but I find this just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1aac548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " valid: (67635, 4), train (608717, 4)\n"
     ]
    }
   ],
   "source": [
    "def partition(df, pct=0.1):\n",
    "    size = int(np.floor(df.shape[0])*0.1)\n",
    "    idx = list(np.random.choice(df.index, size, replace=False))\n",
    "    subset = df.filter(items=idx, axis=0)\n",
    "    rest = df.drop(index = idx)\n",
    "    return subset, rest\n",
    "\n",
    "valdf, traindf = partition(df, 0.2)\n",
    "\n",
    "print(f\" valid: {valdf.shape}, train {traindf.shape}\")\n",
    "traindf.reset_index(inplace=True)\n",
    "valdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e14b13",
   "metadata": {},
   "source": [
    "## the Model\n",
    "The model is fairly simple: 2 embedding layers, \n",
    "    one each for users and movies.\n",
    "    \n",
    "At the end of \"froward\" we simply do the dot product of users and movie embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e8e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_movies = n_movies\n",
    "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
    "        self.movie_emb = nn.Embedding(n_movies, emb_size)\n",
    "        \n",
    "        # initializing our matrices with a positive number generally will yield better results\n",
    "        self.user_emb.weight.data.uniform_(0, 0.5)\n",
    "        self.movie_emb.weight.data.uniform_(0, 0.5)\n",
    "    \n",
    "    def forward(self, users, movies):\n",
    "        m = self.movie_emb(movies)\n",
    "        u = self.user_emb(users)\n",
    "        return (u * m).sum(1)  # taking the dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f59253",
   "metadata": {},
   "source": [
    "## Dataset and dataloader\n",
    "I want to use mini-batch training so we need a dataset\n",
    "and a dataloader.\n",
    "\n",
    "I adapted some code for converting a pandas dataframe into a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3e05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len 608717\n",
      " len 67635\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        print(f\" len {dataframe.shape[0]}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        userid = self.dataframe.userId[index]\n",
    "        movieid = self.dataframe.movieId[index]\n",
    "        rating = np.float32(self.dataframe.rating[index])\n",
    "        return userid, movieid, rating\n",
    "\n",
    "    def __len__(self):\n",
    "        length = self.dataframe.shape[0]\n",
    "        return length\n",
    "                              \n",
    "traindata = CustomDataset(dataframe=traindf)\n",
    "train_dataloader = DataLoader(traindata, batch_size=64)\n",
    "\n",
    "valdata = CustomDataset(dataframe=valdf)\n",
    "val_dataloader = DataLoader(valdata, batch_size=64) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2196c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_model(n_users, n_movies, emb_size=64):\n",
    "    print(f\" emb size {emb_size}\")\n",
    "    n_users = df.userId.max()+1\n",
    "    n_movies = df.movieId.max()+1\n",
    "    model = MF(n_users=n_users, n_movies=n_movies, emb_size=emb_size)\n",
    "    return model\n",
    "\n",
    "# for now I am not experimenting with weight decay.  maybe later.\n",
    "def train_model(model, epochs=2, lr=0.01, wd=0.0, use_cuda=False, print_freq=500):\n",
    "    print(f\"epochs: {epochs}  lr: {lr}\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    if use_cuda:\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "            print(f\"\\tusing cuda = {next(model.parameters()).is_cuda}\")\n",
    "    start_dt = datetime.datetime.now()            \n",
    "    #\n",
    "    # train loop\n",
    "    #\n",
    "    model.train()\n",
    "    train_rmse = []    \n",
    "    val_rmse = []      \n",
    "    for ei in range(epochs):\n",
    "        epoch_mse = 0.0\n",
    "        for tbi, data in enumerate(train_dataloader):\n",
    "            users, movies, ratings = data\n",
    "            if use_cuda:\n",
    "                users = users.cuda()\n",
    "                movies = movies.cuda()\n",
    "                ratings = ratings.cuda()\n",
    "            else:\n",
    "                users = users\n",
    "                movies = movies\n",
    "                ratings = ratings                    \n",
    "            y_hat = model(users, movies)\n",
    "            loss = F.mse_loss(y_hat, ratings)\n",
    "            epoch_mse += loss.item() \n",
    "            optimizer.zero_grad()  # reset gradient\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "        mse = np.round(epoch_mse/len(train_dataloader))\n",
    "        rmse = np.sqrt(mse)\n",
    "        train_rmse.append(rmse)\n",
    "\n",
    "    \n",
    "        #    \n",
    "        # eval loop\n",
    "        #\n",
    "        model.eval()        \n",
    "        epoch_mse = 0.0        \n",
    "        for tbi, data in enumerate(val_dataloader):\n",
    "            users, movies, ratings = data\n",
    "            if use_cuda:\n",
    "                users = users.cuda()\n",
    "                movies = movies.cuda()\n",
    "                ratings = ratings.cuda()\n",
    "            else:\n",
    "                users = users\n",
    "                movies = movies\n",
    "                ratings = ratings \n",
    "            y_hat = model(users, movies)\n",
    "            loss = F.mse_loss(y_hat, ratings)\n",
    "            epoch_mse += loss.item()\n",
    "        mse = np.round(epoch_mse/len(val_dataloader))\n",
    "        rmse = np.sqrt(mse)\n",
    "        val_rmse.append(rmse)\n",
    "        \n",
    "    print(f\"\\tdone with train epochs\")\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"train\"] = train_rmse\n",
    "    df[\"val\"] = val_rmse\n",
    "    df[\"epoch\"] = range(len(train_rmse))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37883adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " emb size 64\n",
      "epochs: 10  lr: 0.005\n",
      "\tusing cuda = True\n",
      "\tdone with train epochs\n",
      " emb size 64\n",
      "epochs: 10  lr: 0.0025\n",
      "\tusing cuda = True\n"
     ]
    }
   ],
   "source": [
    "n_users = df.userId.max()\n",
    "n_movies = df.movieId.max()\n",
    "dflist = []\n",
    "models = {}\n",
    "for epochs in [10]:\n",
    "    for emb_size in [ 64, 128, 256]:\n",
    "        for lr in [0.005, .0025, 0.001]:\n",
    "            model = instantiate_model(n_users=n_users, n_movies=n_movies, emb_size=emb_size)\n",
    "            tdf = train_model(model=model, epochs=epochs, lr=lr, wd=0.0, use_cuda=True, print_freq=5000)\n",
    "            tdf[\"emb_size\"] = emb_size\n",
    "            tdf[\"lr\"] = lr\n",
    "            dflist.append(tdf)\n",
    "            models[emb_size] = model\n",
    "resdf = pd.concat(dflist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ac343",
   "metadata": {},
   "source": [
    "## Plotting the errors\n",
    "I'll use Plotnine, an implmentantion of ggplot, to plot   \n",
    "the errors for each learning rate and embedding size  \n",
    "  for both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff874aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine\n",
    "from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_line, labels\n",
    "\n",
    "\n",
    "trainres = resdf.loc\n",
    "(ggplot(resdf, aes('epoch', 'train',  color='factor(emb_size)'))\n",
    " + geom_line()\n",
    " + labels.ggtitle(\"Train\")\n",
    " + facet_wrap('~lr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7101bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(resdf, aes('epoch', 'val',  color='factor(emb_size)'))\n",
    " + geom_line() \n",
    " + labels.ggtitle(\"Val\")\n",
    " + facet_wrap('~lr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e54ed",
   "metadata": {},
   "source": [
    "# Summary of training\n",
    "* the smallest learning rate seemed to perform best\n",
    "* the larger embedding sizes probably overfit given \n",
    "     with the larger learning rates\n",
    "      although they all seemed fine for the smallest learning rate\n",
    "         for the validation set.\n",
    "\n",
    "So to go forward I would use\n",
    "* learning rate = 0.001\n",
    "* embedding size = 64 or 128\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendations",
   "language": "python",
   "name": "recommendations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
